{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIvhwE4zxX0s",
    "outputId": "f522bfbb-f646-4e60-faff-121b3b8ef927",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\diman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\diman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\diman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Установка нужных пакетов\n",
    "#!pip install --upgrade nltk gensim bokeh umap-learn\n",
    "#!pip install pymorphy2\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import umap\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pymorphy2\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Задание-1:-Перевести-все-слова-в-нижний-регистр-(NLTK)-из-data-и-добавьте-как-лист-токенов-в-листе-data_tok\" data-toc-modified-id=\"Задание-1:-Перевести-все-слова-в-нижний-регистр-(NLTK)-из-data-и-добавьте-как-лист-токенов-в-листе-data_tok-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Задание 1: Перевести все слова в нижний регистр (NLTK) из data и добавьте как лист токенов в листе data_tok</a></span></li><li><span><a href=\"#Задание-2:-Подсчитайте-топ10-самых-популярных-лем-в-рамках-data\" data-toc-modified-id=\"Задание-2:-Подсчитайте-топ10-самых-популярных-лем-в-рамках-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Задание 2: Подсчитайте топ10 самых популярных лем в рамках data</a></span></li><li><span><a href=\"#Задание-3:-Подсчитайте-количество-разных-слов-до-и-после-лемматизации\" data-toc-modified-id=\"Задание-3:-Подсчитайте-количество-разных-слов-до-и-после-лемматизации-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Задание 3: Подсчитайте количество разных слов до и после лемматизации</a></span></li><li><span><a href=\"#Задание-4:-Подсчитайте-количество-разных-слов-до-и-после-стемминга\" data-toc-modified-id=\"Задание-4:-Подсчитайте-количество-разных-слов-до-и-после-стемминга-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Задание 4: Подсчитайте количество разных слов до и после стемминга</a></span></li><li><span><a href=\"#Задание-5:-сделать-на-датасете-data-сначала-лемматизацию,-потом-стемминг-и-наоборот\" data-toc-modified-id=\"Задание-5:-сделать-на-датасете-data-сначала-лемматизацию,-потом-стемминг-и-наоборот-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Задание 5: сделать на датасете data сначала лемматизацию, потом стемминг и наоборот</a></span></li><li><span><a href=\"#Задание-6:\" data-toc-modified-id=\"Задание-6:-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Задание 6:</a></span></li><li><span><a href=\"#Задание-7:\" data-toc-modified-id=\"Задание-7:-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Задание 7:</a></span></li><li><span><a href=\"#Задание-8:\" data-toc-modified-id=\"Задание-8:-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Задание 8:</a></span></li><li><span><a href=\"#Задание-9:\" data-toc-modified-id=\"Задание-9:-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Задание 9:</a></span></li><li><span><a href=\"#Bag-Of-Words-(BOW)\" data-toc-modified-id=\"Bag-Of-Words-(BOW)-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Bag Of Words (BOW)</a></span></li></ul></div>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Домашнее задание на HR:**\n",
    "\n",
    "**ML1_1:**\n",
    "\n",
    "https://www.hackerrank.com/challenges/capturing-non-capturing-groups/problem?isFullScreen=true"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Regex_Pattern = r'okokok(ok)*'\n",
    "Regex_Pattern = r'(ok){3,}'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ML1_2:**\n",
    "\n",
    "https://www.hackerrank.com/challenges/branch-reset-groups/problem?isFullScreen=true"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Regex_Pattern = r'(^(\\d\\d---){3}\\d\\d$)|(^(\\d\\d-){3}\\d\\d$)|(^(\\d\\d:){3}\\d\\d$)|(^(\\d\\d\\.){3}\\d\\d$)'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ML1_3:**\n",
    "\n",
    "https://www.hackerrank.com/challenges/detect-html-links/problem?isFullScreen=true\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r'<a href=\"(.*?)\".*?>([^<>].*?)?<\\/.?>'\n",
    "answer = []\n",
    "for i in range(int(input())):\n",
    "    txt = input()\n",
    "    if re.findall(pattern, txt):\n",
    "        for j in re.findall(pattern, txt):\n",
    "            answer.append(j)\n",
    "for i in answer:\n",
    "    print(i[0].strip() + ',' + i[1].strip())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hF9WPCtfxZR9",
    "outputId": "e284157f-d7b7-44b6-f8a4-8eadef6a2859",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# выгружаем датасет:\n",
    "#!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "MaFpN9pvxtNg",
    "outputId": "3dc48564-f850-49b8-f597-f5349c320bff",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"What TV shows or books help you read people's body language?\\n\""
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(open(os.path.join('data', 'quora.txt'), encoding=\"utf-8\"))\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HvXRbOKGx0l_",
    "outputId": "a92652c2-e5c7-4d7a-9b39-9869f8aa7141",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'TV', 'shows', 'or', 'books', 'help', 'you', 'read', 'people', \"'\", 's', 'body', 'language', '?']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(data[50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovkxi_QOySCl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Задание 1: Перевести все слова в нижний регистр (NLTK) из data и добавьте как лист токенов в листе data_tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EK7uvHi6zeWY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_lower = [i.lower() for i in data]\n",
    "data_tok = [tokenizer.tokenize(i) for i in data_lower]\n",
    "\n",
    "\n",
    "#checking\n",
    "\n",
    "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
    "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtKeoLCYzY4j",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Задание 2: Подсчитайте топ10 самых популярных лем в рамках data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in data_tok:\n",
    "    for j in i:\n",
    "        all_words.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_lem = [lemmatizer.lemmatize(i) for i in all_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1gkniqr1myx5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cnt_lem = dict()\n",
    "for i in all_lem:\n",
    "    cnt_lem[i] = cnt_lem.get(i, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WmyIMwouk8O3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "j7xzW0xplh2h",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'words': cnt_lem.keys(),'cnt': cnt_lem.values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "a_BxzSv9yR0w",
    "outputId": "f638908f-0617-4341-a35e-81fcf7549473",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   words     cnt\n17     ?  552413\n30   the  252068\n18  what  214798\n10    is  185392\n24     a  172513\n1      i  149873\n22    to  141788\n37    in  139813\n60   how  135687\n49    of  112001",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>words</th>\n      <th>cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17</th>\n      <td>?</td>\n      <td>552413</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>the</td>\n      <td>252068</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>what</td>\n      <td>214798</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>is</td>\n      <td>185392</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>a</td>\n      <td>172513</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i</td>\n      <td>149873</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>to</td>\n      <td>141788</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>in</td>\n      <td>139813</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>how</td>\n      <td>135687</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>of</td>\n      <td>112001</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('cnt', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1SM3sn1zf1b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Задание 3: Подсчитайте количество разных слов до и после лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q88BIteDzpWR",
    "outputId": "d35882ec-950a-4bec-c33b-3a70c2de44b2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "87819"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = list(set(all_words))\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tPougk36s1-L",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unique_lem = list(set(all_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "80303"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxKa8yUUzqNN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Задание 4: Подсчитайте количество разных слов до и после стемминга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "x91DX51qzszR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unique_stem = [ps.stem(i) for i in unique_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JpkDts_wtd3Y",
    "outputId": "d6b5cad9-15af-4935-b68b-883c758ecc97",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "67026"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(unique_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "lO0igs_2t8n0",
    "outputId": "03e59ad0-d461-4434-c33f-ba8959b4b67d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'catelyn'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_stem[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'catelyn'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXA7Fe_izuqh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Задание 5: сделать на датасете data сначала лемматизацию, потом стемминг и наоборот\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BGgmHzUAzwqO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "7131345"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_after_lem = [ps.stem(i) for i in all_lem]\n",
    "len(stem_after_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_stem = [ps.stem(i) for i in all_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "7131345"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_after_stem = [lemmatizer.lemmatize(i) for i in all_stem]\n",
    "len(lem_after_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Изначальное количество всех слов с повторениями: 7131345\n",
      "Количество всех слов после леммитизации: 7131345\n",
      "Количество всех слов после стемминга: 7131345\n",
      "Количество всех слов после стемминга, произведенного после леммитизации: 7131345\n",
      "Количество всех слов после леммитизации, произведенного после стемминга: 7131345\n",
      "Количество уникальных слов: 87819\n",
      "Количество уникальных лемм после леммитизации: 80303\n",
      "Количество уникальных стемм после стемминга: 67026\n",
      "Количество уникальных стемм после стемминга, произведенного после леммитизации: 66835\n",
      "Количество уникальных лемм после леммитизации, произведенного после стемминга: 66818\n"
     ]
    }
   ],
   "source": [
    "print(f'Изначальное количество всех слов с повторениями: {len(all_words)}',\n",
    "      f'Количество всех слов после леммитизации: {len(all_lem)}',\n",
    "      f'Количество всех слов после стемминга: {len(all_stem)}',\n",
    "      f'Количество всех слов после стемминга, произведенного после леммитизации: {len(stem_after_lem)}',\n",
    "      f'Количество всех слов после леммитизации, произведенного после стемминга: {len(lem_after_stem)}',\n",
    "      f'Количество уникальных слов: {len(set(all_words))}',\n",
    "      f'Количество уникальных лемм после леммитизации: {len(set(all_lem))}',\n",
    "      f'Количество уникальных стемм после стемминга: {len(set(all_stem))}',\n",
    "      f'Количество уникальных стемм после стемминга, произведенного после леммитизации: {len(set(stem_after_lem))}',\n",
    "      f'Количество уникальных лемм после леммитизации, произведенного после стемминга: {len(set(lem_after_stem))}',\n",
    "      sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'business'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words[555]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'business'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lem[555]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'busi'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_after_lem[555]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'busi'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stem[555]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'busi'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_after_stem[555]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At9iloRCVShn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "REGEXP\n",
    "\n",
    "https://www.programiz.com/python-programming/regex \n",
    "\n",
    "https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "521aLisyVUg_",
    "outputId": "fd21b30e-aadf-47eb-e776-ba59ff427d7f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search unsuccessful.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = 'a*s'\n",
    "test_string = 'abyss'\n",
    "result = re.match(pattern, test_string)\n",
    "\n",
    "if result:\n",
    "    print(\"Search successful.\")\n",
    "else:\n",
    "    print(\"Search unsuccessful.\")\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH7qx_irU4Y8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Задание 6: \n",
    "https://www.hackerrank.com/challenges/matching-specific-string/problem?isFullScreen=true \n",
    "\n",
    "**Regex_Pattern = r'hackerrank'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# Задание 7: \n",
    "https://www.hackerrank.com/challenges/matching-whitespace-non-whitespace-character/problem?isFullScreen=true\n",
    "\n",
    "**Regex_Pattern = r\"\\S\\S\\s\\S\\S\\s\\S\\S\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Задание 8: \n",
    "https://www.hackerrank.com/challenges/matching-start-end/problem?isFullScreen=true\n",
    "\n",
    "**Regex_Pattern = r\"^\\d\\w{4}\\.$\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Задание 9: \n",
    "https://www.hackerrank.com/challenges/matching-word-boundaries/problem?isFullScreen=true\n",
    "\n",
    "**Regex_Pattern = r'\\b[aeiouAEIOU][a-zA-Z]*\\b'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Csv2YN2IRXJB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Bag Of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8YWG3JhSFeZ",
    "outputId": "325b4fa5-7d56-4993-b4d0-1aae78bad94e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'learning']\n",
      "['learning', 'is', 'a', 'good', 'practice']\n",
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'is', 'a', 'good', 'practice']\n",
      "['welcome', 'great', 'learning', 'now', 'start', 'good', 'practice']\n",
      "[1, 1, 2, 1, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def vectorize(tokens):\n",
    "    ''' This function takes list of words in a sentence as input \n",
    "    and returns a vector of size of filtered_vocab.It puts 0 if the \n",
    "    word is not present in tokens and count of token if present.'''\n",
    "    vector=[]\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "def unique(sequence):\n",
    "    '''This functions returns a list in which the order remains \n",
    "    same and no item repeats.Using the set() function does not \n",
    "    preserve the original ordering,so i didnt use that instead'''\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "#create a list of stopwords.You can import stopwords from nltk too\n",
    "stopwords=[\"to\",\"is\",\"a\"]\n",
    "\n",
    "#list of special characters.You can use regular expressions too\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
    "\n",
    "#Write the sentences in the corpus,in our case, just two \n",
    "string1=\"Welcome to Great Learning , Now start learning\"\n",
    "string2=\"Learning is a good practice\"\n",
    "\n",
    "#convert them to lower case\n",
    "string1=string1.lower()\n",
    "string2=string2.lower()\n",
    "\n",
    "#split the sentences into tokens\n",
    "tokens1=string1.split()\n",
    "tokens2=string2.split()\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "\n",
    "#create a vocabulary list\n",
    "vocab=unique(tokens1+tokens2)\n",
    "print(vocab)\n",
    "\n",
    "#filter the vocabulary list\n",
    "filtered_vocab=[]\n",
    "for w in vocab: \n",
    "    if w not in stopwords and w not in special_char: \n",
    "        filtered_vocab.append(w)\n",
    "print(filtered_vocab)\n",
    "\n",
    "#convert sentences into vectords\n",
    "vector1=vectorize(tokens1)\n",
    "print(vector1)\n",
    "vector2=vectorize(tokens2)\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "['welcome', 'great', 'learning', 'now', 'start', 'good', 'practice']"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOZ1qx05Q46b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Задание 10: Реализовать Bag of words на data_tok (можно на NLTK, можно без)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "Tew2nQN4OCiW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize(tokens):\n",
    "    ''' This function takes list of words in a sentence as input\n",
    "    and returns a vector of size of filtered_vocab.It puts 0 if the\n",
    "    word is not present in tokens and count of token if present.'''\n",
    "    vector=[]\n",
    "    for w in filtered_vocab2:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "def unique(sequence):\n",
    "    '''This functions returns a list in which the order remains\n",
    "    same and no item repeats.Using the set() function does not\n",
    "    preserve the original ordering,so i didnt use that instead'''\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "#create a list of stopwords.You can import stopwords from nltk too\n",
    "stopwords=[\"to\",\"is\",\"a\"]\n",
    "\n",
    "#list of special characters.You can use regular expressions too\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "all_words2 = []\n",
    "for i in data_tok[:300]:\n",
    "    for j in i:\n",
    "        all_words2.append(j)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "vocab2 = unique(all_words2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "filtered_vocab2 = []\n",
    "for w in vocab2:\n",
    "    if w not in stopwords and w not in special_char:\n",
    "        filtered_vocab2.append(w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "vector_of_data_tok2 = []\n",
    "for i in data_tok[:300]:\n",
    "    vector_of_data_tok2.append(vectorize(i))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "1348"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_of_data_tok2[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "1348"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_vocab2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim = dot(vector_of_data_tok2[0], vector_of_data_tok2[1])/(norm(vector_of_data_tok2[0])*norm(vector_of_data_tok2[1]))\n",
    "cos_sim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7715167498104595\n",
      "What are the components of fitness?\n",
      "\n",
      "What are the main components of a cologne?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_cos = 0\n",
    "v1 = None\n",
    "v2 = None\n",
    "pos1 = None\n",
    "pos2 = None\n",
    "for i in range(len(vector_of_data_tok2)):\n",
    "    for j in range(i+1, len(vector_of_data_tok2)-1):\n",
    "        cos = dot(vector_of_data_tok2[i], vector_of_data_tok2[j])/(norm(vector_of_data_tok2[i])*norm(vector_of_data_tok2[j]))\n",
    "        if max_cos < cos:\n",
    "            max_cos = cos\n",
    "            v1 = vector_of_data_tok2[i]\n",
    "            v2 = vector_of_data_tok2[j]\n",
    "            pos1 = i\n",
    "            pos2 = j\n",
    "print(max_cos)\n",
    "print(data[pos1], data[pos2], sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Задание 11: Реализовать stemming, lemmatization & BoW на следующем датасете:\n",
    "https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(os.path.join('data', 'labeled.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "comment = data2['comment'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "comment = [i.lower().strip() for i in comment]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "comment_tok = [tokenizer.tokenize(i) for i in comment]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "all_words_in_com = []\n",
    "for i in comment_tok:\n",
    "    for j in i:\n",
    "        all_words_in_com.append(j)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**stemming**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "ru_stem = SnowballStemmer(language=\"russian\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "stem_com = [ru_stem.stem(i) for i in all_words_in_com]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "umique_stem_com = set(stem_com)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "33642"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(umique_stem_com)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "'шапк'"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_com[123]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**lemmatization**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "lem_com = [morph.parse(i)[0].normal_form for i in all_words_in_com]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "umique_lem_com = set(lem_com)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "35315"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(umique_lem_com)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "data": {
      "text/plain": "'шапка'"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_com[123]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "'шапке'"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_in_com[123]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**BoW.** Основой послужил текст после леммитизации и стемминга"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "comment_tok_stem_after_lem = []\n",
    "for i in comment_tok:\n",
    "    comment_tok_stem_after_lem.append([ru_stem.stem(morph.parse(a)[0].normal_form) for a in i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comment_tok[2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comment_tok_stem_after_lem[2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def vectorize(tokens):\n",
    "    ''' This function takes list of words in a sentence as input\n",
    "    and returns a vector of size of filtered_vocab.It puts 0 if the\n",
    "    word is not present in tokens and count of token if present.'''\n",
    "    vector = []\n",
    "    for w in filtered_vocab3:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "\n",
    "\n",
    "def unique(sequence):\n",
    "    '''This functions returns a list in which the order remains\n",
    "    same and no item repeats.Using the set() function does not\n",
    "    preserve the original ordering,so i didnt use that instead'''\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "#create a list of stopwords.You can import stopwords from nltk too\n",
    "stopwrds = stopwords.words('russian')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "#list of special characters.You can use regular expressions too\n",
    "special_char = [\",\", \":\", \" \", \";\", \".\", \"?\", '=', '!', '-', '...', '/']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "all_words_in_com2 = []\n",
    "for i in comment_tok_stem_after_lem[:1000]:\n",
    "    for j in i:\n",
    "        all_words_in_com2.append(j)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "vocab3 = unique(all_words_in_com2)\n",
    "filtered_vocab3 = []\n",
    "for w in vocab3:\n",
    "    if w not in stopwrds and w not in special_char:\n",
    "        filtered_vocab3.append(w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "vector_of_comment_tok = []\n",
    "for i in comment_tok_stem_after_lem[:1000]:\n",
    "    vector_of_comment_tok.append(vectorize(i))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Попробуем найти масимально похожие комментарии**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5773502691896258\n",
      "я вот не пойму, я у тебя совета что-ли из раза в раз прошу?\n",
      "надеюсь, ты понял, о чем просят тебя написать.\n"
     ]
    }
   ],
   "source": [
    "max_cos = 0\n",
    "v1 = None\n",
    "v2 = None\n",
    "pos1 = None\n",
    "pos2 = None\n",
    "for i in range(len(vector_of_comment_tok)):\n",
    "    for j in range(i+1, len(vector_of_comment_tok)-1):\n",
    "        cos = dot(vector_of_comment_tok[i], vector_of_comment_tok[j])/(norm(vector_of_comment_tok[i])*norm(vector_of_comment_tok[j]))\n",
    "        if max_cos < cos:\n",
    "            max_cos = cos\n",
    "            v1 = vector_of_comment_tok[i]\n",
    "            v2 = vector_of_comment_tok[j]\n",
    "            pos1 = i\n",
    "            pos2 = j\n",
    "print(max_cos)\n",
    "print(comment[pos1], comment[pos2], sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "print(pos1, pos2, sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Вывод:** На этом датасете качество определения схожих комментариев не очень высокое. Как видно, комментарии похожи друг на друга очень отдаленно, только составом слов"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}